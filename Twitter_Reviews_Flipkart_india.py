# -*- coding: utf-8 -*-
"""Copy of Twitter_Flipkart India.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SylPED3UVAuIXCkoktVilmqRZDQigOrn
"""

# Install Libraries
!pip install textblob
!pip install tweepy
!pip install WordCloud

import tweepy
from textblob import TextBlob
from wordcloud import WordCloud
import pandas as pd
import numpy as np
import re
import matplotlib.pyplot as plt
plt.style.use('fivethirtyeight')

#all 4 authentication keys to access twitter API
# to connect as OATH handler or jump server / revers proxy server
consumer_key = "xxxxxxxxxxxxxxxxxxxxxxxx"
consumer_sec = "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"

# from proxy server we need to connect
access_token = "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"
access_token_sec = "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"

#Create the authentication object
authenticate = tweepy.OAuthHandler(consumer_key, consumer_sec)

#Set the access token and access token secret
authenticate.set_access_token(access_token,access_token_sec)

#Create the API object while passing in the auth information
api = tweepy.API(authenticate, wait_on_rate_limit = True)

#Create the API object while passing in the auth information
api_connect= tweepy.API(authenticate)

#Extract 3000 tweets from the twitter user
tweet_data = api_connect.search('Flipkart India', count=50000, lang = "en", tweet_mode="extended")

#Print the last 5 Tweets fromt the account
print("Show the 5 recent tweets: \n")
i = 1
for tweet in tweet_data[0:5]:
    print(str(i) + ') '+ tweet.full_text + '\n')
    i = i = 1

#Create a dataframe with a column called Tweets
df =pd.DataFrame([tweet.full_text for tweet in tweet_data], columns=['Tweet'])

#Show the first 5 rows of the data
df.head()

#Create a function to clean the tweets
# Removing punctuations and symbols
# punctuations = ' ' ' ! ()-[]{};:' " \,<>./?@#$%^&*_~' ' '

def cleanTxt(text):
    text =re.sub(r'@[A-Za-z0-9]+', '', text) # removed @mentions
    text =re.sub(r'#', '', text) #removing symbols
    text =re.sub(r'RT @', '', text) #removing symbols
    text =re.sub(r':', '', text) #removing symbols
    text =re.sub(r';', '', text) #removing symbols
    text =re.sub(r'"', '', text) #removing symbols
    text =re.sub(r'\n', '', text)#removing symbols
    text =re.sub(r'_', '', text) #removing symbols
    text =re.sub(r'//t', '', text) #removing symbols
    text =re.sub(r'/', '', text) #removing symbols
    text =re.sub(r'-', '', text) #removing symbols
    text =re.sub(r'&', '', text) #removing symbols
    text =re.sub(r'RT[\s]+','',text) #removing RT
    text =re.sub(r'https', '', text) #removing symbols
    text =re.sub(r'https?:\/\/\S+', '', text) #removing the hyper link
    
    return text

#Cleaning the text
df['Tweet']= df['Tweet'].apply(cleanTxt)

#Show the cleaned test
df

print(df)

import nltk
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('stopwords')
from nltk.tokenize import LineTokenizer, SpaceTokenizer, TweetTokenizer
from nltk import word_tokenize
from nltk import sent_tokenize, word_tokenize
from nltk.stem.snowball import SnowballStemmer
from nltk.stem.wordnet import WordNetLemmatizer
from nltk.corpus import stopwords

from nltk.corpus import stopwords
stop_words = set(stopwords.words('english'))
print(stop_words)
words = word_tokenize('df')
filtered_tweet = []
for w in words:
    if w not in stop_words:
        filtered_tweet.append(w)

print(filtered_tweet)

from nltk.stem import PorterStemmer
stemmer = PorterStemmer()
input_str=('df')
input_str=nltk.word_tokenize(input_str)
for word in input_str:
    print(stemmer.stem(word))

from nltk.stem import WordNetLemmatizer
lemmatizer=WordNetLemmatizer()
input_str=('df')
input_str=nltk.word_tokenize(input_str)
for word in input_str:
    print(lemmatizer.lemmatize(word))

#Create a function to get the subjectivity
def getSubjectivity(text):
    return TextBlob(text).sentiment.subjectivity

#Create a function to get the polarity
def getPolarity(text):
    return TextBlob(text).sentiment.polarity

#Create two new columns
df['Subjectivity'] = df['Tweet'].apply(getSubjectivity)
df['Polarity'] = df['Tweet'].apply(getPolarity)

#Show the new dataframe with the new columns
df

#Plot The WordCloud
allWords = ' '.join([ tweet for tweet in df['Tweet']])
wordcloud = WordCloud(width =500, height=300, random_state=21, max_font_size=119).generate(allWords)

plt.imshow(wordcloud, interpolation = "bilinear")
plt.axis('off')
plt.show()

#Create a function to compute the negative, neutral and positive analysis
def getAnalysis(score):
    if score<0:
        return 'Negative'
    elif score == 0:
        return 'Neutral'
    else:
        return 'Positive'

df['Analysis'] = df['Polarity'].apply(getAnalysis)

#show the dataframe
df

import matplotlib.pyplot as plt
import seaborn as sns

sns.distplot(df['Polarity'],kde=False)

sns.distplot(df['Subjectivity'],kde=False)

#Print all of the positive tweets
j=1
sortedDF =df.sort_values(by=['Polarity'])
for i in range(0,sortedDF.shape[0]):
    if(sortedDF['Analysis'][i] ==['Positive']):
        print(str(j)+ ') '+sortedDF['Tweet'][i])
        print()
        j = j+1

#Print the negative tweets
j=1
sortedDF =df.sort_values(by=['Polarity'], ascending='False')
for i in range(0,sortedDF.shape[0]):
    if (sortedDF['Analysis'][i] =='Negative'):
        print(str(j) +') '+sortedDF['Tweet'][i])
        print()
        j = j+1

sns.countplot(x="Analysis",data=df)

#Plot the polarity and subjectivity
plt.scatter(df['Polarity'],df['Subjectivity'])
plt.title('Sentiment Analysis')
plt.xlabel('Polarity')
plt.ylabel('Subjectivity')
plt.show()

sns.lmplot(x="Polarity", y="Subjectivity", data=df, hue='Analysis',palette="Set1")

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score,f1_score#hamming_loss,zero_one_less
from sklearn.metrics import confusion_matrix
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np
import pdb

tfidf = TfidfVectorizer(max_features=10000)
X=df['Tweet']
y=df['Analysis']
X=tfidf.fit_transform(X)
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=0)

from sklearn.svm import LinearSVC
clf = LinearSVC()
clf.fit(X_train,y_train)

from sklearn.metrics import classification_report
y_pred=clf.predict(X_test)
print(classification_report(y_test,y_pred))